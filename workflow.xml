<workflow-app name='test workfow' xmlns="uri:oozie:workflow:0.5">
    <start to='spark_test'/>
    <action name='spark_test'>
         <!--<spark xmlns="uri:oozie:spark-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>oozie.launcher.mapreduce.job.queue.name</name>
                    <value>${queue_name}</value>
                </property>
                <property>
                    <name>mapreduce.map.memory.mb</name>
                    <value>1024</value>
                </property>
            </configuration>
            <master>${master}</master>
            <mode>${mode}</mode>
            <name>test</name>
            <jar>${app_path}/jobs/test_udf.py</jar>
            <spark-opts> -/-master yarn-cluster -/-queue=${queueName} -/-conf spark.scheduler.mode=FAIR -/-driver-memory=1g -/-executor-memory=1g -/-num-executors=2</spark-opts>
        </spark>!-->
      <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.job.queue.name</name>
                    <value>${queue_name}</value>
                </property>
                <property>
                    <name>mapreduce.map.memory.mb</name>
                    <value>1024</value>
                </property>
            </configuration>
            <exec>test.sh</exec>
            <file>${app_path}/test/test.sh</file>
        </shell>

        <ok to="End"/>
        <error to="Kill"/>
    </action>
    <kill name='Kill'>
        <message>shell action failed: ${wf:errorCode('wordcount')}</message>
    </kill>
    <end name='End'/>
</workflow-app>

  <!-- !-->